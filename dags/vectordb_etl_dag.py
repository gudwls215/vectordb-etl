"""
VectorDB ETL Airflow DAG
HTML ë¬¸ì„œë¥¼ Milvus ë²¡í„° DBì— ì €ì¥í•˜ëŠ” ETL íŒŒì´í”„ë¼ì¸

DAG êµ¬ì¡°:
    extract_html_documents -> transform_to_chunks -> load_to_milvus -> validate_quality

ì‚¬ìš©ë²•:
    1. ì´ íŒŒì¼ì„ Airflow dags ë””ë ‰í† ë¦¬ì— ë³µì‚¬
    2. VECTORDB_ETL_PATH í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë˜ëŠ” ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
    3. Airflow UIì—ì„œ DAG í™œì„±í™”
"""

from datetime import datetime, timedelta
from typing import Any, Dict, List
import os
import pickle
import json

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.utils.dates import days_ago

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
VECTORDB_ETL_PATH = os.environ.get(
    "VECTORDB_ETL_PATH", 
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)

# ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
import sys
if VECTORDB_ETL_PATH not in sys.path:
    sys.path.insert(0, VECTORDB_ETL_PATH)


# DAG ê¸°ë³¸ ì¸ì
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "execution_timeout": timedelta(hours=2),
}


def extract_html_documents(**context) -> str:
    """
    Extract ë‹¨ê³„: HTML íŒŒì¼ì—ì„œ ë¬¸ì„œ ì¶”ì¶œ
    
    Returns:
        ì €ì¥ëœ ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
    """
    from modules import load_html_documents, get_config, DATA_DIR
    
    config = get_config()
    
    print(f"HTML ë””ë ‰í† ë¦¬: {config.html_dir}")
    print(f"íŒ¨í„´: {config.html_glob_pattern}")
    
    # HTML ë¬¸ì„œ ë¡œë“œ
    documents = load_html_documents(
        directory=config.html_dir,
        glob_pattern=config.html_glob_pattern,
        config=config
    )
    
    print(f"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
    
    # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
    documents_path = os.path.join(DATA_DIR, "documents.pkl")
    os.makedirs(DATA_DIR, exist_ok=True)
    
    with open(documents_path, 'wb') as f:
        pickle.dump(documents, f)
    
    # XComìœ¼ë¡œ ê²½ë¡œ ì „ë‹¬
    context['ti'].xcom_push(key='documents_path', value=documents_path)
    context['ti'].xcom_push(key='document_count', value=len(documents))
    
    return documents_path


def transform_to_chunks(**context) -> str:
    """
    Transform ë‹¨ê³„: ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
    
    Returns:
        ì €ì¥ëœ ì²­í¬ íŒŒì¼ ê²½ë¡œ
    """
    from modules import chunk_documents, get_config, DATA_DIR
    
    config = get_config()
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    documents_path = context['ti'].xcom_pull(
        key='documents_path', 
        task_ids='extract_html_documents'
    )
    
    with open(documents_path, 'rb') as f:
        documents = pickle.load(f)
    
    print(f"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
    
    # ì²­í‚¹
    chunks = chunk_documents(
        documents,
        config=config.chunker,
        remove_duplicates=True,
        similarity_threshold=config.duplicate_similarity_threshold
    )
    
    print(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
    
    # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
    chunks_path = os.path.join(DATA_DIR, "chunks.pkl")
    
    with open(chunks_path, 'wb') as f:
        pickle.dump(chunks, f)
    
    # XComìœ¼ë¡œ ê²½ë¡œ ì „ë‹¬
    context['ti'].xcom_push(key='chunks_path', value=chunks_path)
    context['ti'].xcom_push(key='chunk_count', value=len(chunks))
    
    return chunks_path


def load_to_milvus(**context) -> Dict[str, Any]:
    """
    Load ë‹¨ê³„: Milvusì— ë²¡í„° ì €ì¥
    
    Returns:
        ì €ì¥ ê²°ê³¼ í†µê³„
    """
    from modules import get_vector_store, DATA_DIR
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    chunks_path = context['ti'].xcom_pull(
        key='chunks_path', 
        task_ids='transform_to_chunks'
    )
    
    with open(chunks_path, 'rb') as f:
        chunks = pickle.load(f)
    
    print(f"ë¡œë“œëœ ì²­í¬ ìˆ˜: {len(chunks)}")
    
    # Milvusì— ì €ì¥
    vectorstore = get_vector_store()
    vectorstore.create_collection(drop_existing=True)
    inserted_count = vectorstore.insert_documents(chunks)
    
    # í†µê³„ ì¡°íšŒ
    stats = vectorstore.get_collection_stats()
    
    result = {
        "inserted_count": inserted_count,
        "collection_name": stats.get("collection_name"),
        "total_vectors": stats.get("row_count"),
    }
    
    print(f"ì €ì¥ ì™„ë£Œ: {result}")
    
    # XComìœ¼ë¡œ ê²°ê³¼ ì „ë‹¬
    context['ti'].xcom_push(key='load_stats', value=result)
    
    return result


def validate_quality(**context) -> Dict[str, Any]:
    """
    Validate ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦
    
    Returns:
        ê²€ì¦ ë³´ê³ ì„œ
    """
    from modules import get_vector_store, validate_pipeline, DATA_DIR
    
    # ì²­í¬ ë¡œë“œ
    chunks_path = context['ti'].xcom_pull(
        key='chunks_path', 
        task_ids='transform_to_chunks'
    )
    
    with open(chunks_path, 'rb') as f:
        chunks = pickle.load(f)
    
    # ë²¡í„° ì €ì¥ì†Œ
    vectorstore = get_vector_store()
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_queries = [
        "ì„œìš¸ ì‚¬ë¬´ì‹¤ ì£¼ì†Œ",
        "ìˆ˜ê°•ì‹ ì²­ë°©ë²•",
    ]
    
    # ê²€ì¦
    report = validate_pipeline(
        vectorstore=vectorstore,
        chunks=chunks,
        test_queries=test_queries,
        sample_count=3
    )
    
    # ë³´ê³ ì„œ ì €ì¥
    report_path = os.path.join(
        DATA_DIR, 
        f"validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ê²€ì¦ ë³´ê³ ì„œ ì €ì¥: {report_path}")
    
    # XComìœ¼ë¡œ ê²°ê³¼ ì „ë‹¬
    context['ti'].xcom_push(key='validation_report', value=report)
    context['ti'].xcom_push(key='report_path', value=report_path)
    
    return report


def notify_completion(**context):
    """
    íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ì•Œë¦¼
    """
    # ëª¨ë“  ë‹¨ê³„ì˜ ê²°ê³¼ ìˆ˜ì§‘
    document_count = context['ti'].xcom_pull(
        key='document_count', 
        task_ids='extract_html_documents'
    )
    
    chunk_count = context['ti'].xcom_pull(
        key='chunk_count', 
        task_ids='transform_to_chunks'
    )
    
    load_stats = context['ti'].xcom_pull(
        key='load_stats', 
        task_ids='load_to_milvus'
    )
    
    validation_report = context['ti'].xcom_pull(
        key='validation_report', 
        task_ids='validate_quality'
    )
    
    # ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š VectorDB ETL íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ìš”ì•½")
    print("=" * 60)
    print(f"ë¬¸ì„œ ìˆ˜: {document_count}")
    print(f"ì²­í¬ ìˆ˜: {chunk_count}")
    print(f"ì €ì¥ëœ ë²¡í„° ìˆ˜: {load_stats.get('total_vectors', 'N/A')}")
    print(f"ëª©í‘œ ë²”ìœ„ ë‚´ ì²­í¬ ë¹„ìœ¨: {validation_report.get('in_range_ratio', 'N/A'):.1f}%")
    print("=" * 60)
    
    # ì—¬ê¸°ì— Slack, ì´ë©”ì¼ ë“± ì•Œë¦¼ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥


# DAG ì •ì˜
with DAG(
    dag_id="vectordb_etl_pipeline",
    default_args=default_args,
    description="HTML ë¬¸ì„œë¥¼ Milvus ë²¡í„° DBì— ì €ì¥í•˜ëŠ” ETL íŒŒì´í”„ë¼ì¸",
    schedule_interval="@daily",  # ë§¤ì¼ ì‹¤í–‰ (í•„ìš”ì— ë”°ë¼ ì¡°ì •)
    start_date=days_ago(1),
    catchup=False,
    tags=["vectordb", "etl", "milvus", "embedding"],
    doc_md=__doc__,
) as dag:
    
    # ì‹œì‘ íƒœìŠ¤í¬
    start = EmptyOperator(task_id="start")
    
    # Extract íƒœìŠ¤í¬
    extract_task = PythonOperator(
        task_id="extract_html_documents",
        python_callable=extract_html_documents,
        provide_context=True,
    )
    
    # Transform íƒœìŠ¤í¬
    transform_task = PythonOperator(
        task_id="transform_to_chunks",
        python_callable=transform_to_chunks,
        provide_context=True,
    )
    
    # Load íƒœìŠ¤í¬
    load_task = PythonOperator(
        task_id="load_to_milvus",
        python_callable=load_to_milvus,
        provide_context=True,
    )
    
    # Validate íƒœìŠ¤í¬
    validate_task = PythonOperator(
        task_id="validate_quality",
        python_callable=validate_quality,
        provide_context=True,
    )
    
    # ì™„ë£Œ ì•Œë¦¼ íƒœìŠ¤í¬
    notify_task = PythonOperator(
        task_id="notify_completion",
        python_callable=notify_completion,
        provide_context=True,
    )
    
    # ì¢…ë£Œ íƒœìŠ¤í¬
    end = EmptyOperator(task_id="end")
    
    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì •ì˜
    start >> extract_task >> transform_task >> load_task >> validate_task >> notify_task >> end


# ê°œë³„ ë‹¨ê³„ ì‹¤í–‰ì„ ìœ„í•œ ì„œë¸Œ DAGë“¤
# í•„ìš”ì‹œ ë³„ë„ DAGë¡œ ë¶„ë¦¬í•˜ì—¬ ë…ë¦½ ì‹¤í–‰ ê°€ëŠ¥

with DAG(
    dag_id="vectordb_etl_extract_only",
    default_args=default_args,
    description="HTML ë¬¸ì„œ ì¶”ì¶œë§Œ ì‹¤í–‰",
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰
    start_date=days_ago(1),
    catchup=False,
    tags=["vectordb", "etl", "extract"],
) as extract_dag:
    
    extract_only_task = PythonOperator(
        task_id="extract_html_documents",
        python_callable=extract_html_documents,
        provide_context=True,
    )


with DAG(
    dag_id="vectordb_etl_transform_only",
    default_args=default_args,
    description="ë¬¸ì„œ ì²­í‚¹ë§Œ ì‹¤í–‰ (extract ê²°ê³¼ í•„ìš”)",
    schedule_interval=None,
    start_date=days_ago(1),
    catchup=False,
    tags=["vectordb", "etl", "transform"],
) as transform_dag:
    
    transform_only_task = PythonOperator(
        task_id="transform_to_chunks",
        python_callable=transform_to_chunks,
        provide_context=True,
    )


with DAG(
    dag_id="vectordb_etl_load_only",
    default_args=default_args,
    description="Milvus ì €ì¥ë§Œ ì‹¤í–‰ (transform ê²°ê³¼ í•„ìš”)",
    schedule_interval=None,
    start_date=days_ago(1),
    catchup=False,
    tags=["vectordb", "etl", "load"],
) as load_dag:
    
    load_only_task = PythonOperator(
        task_id="load_to_milvus",
        python_callable=load_to_milvus,
        provide_context=True,
    )


with DAG(
    dag_id="vectordb_etl_validate_only",
    default_args=default_args,
    description="í’ˆì§ˆ ê²€ì¦ë§Œ ì‹¤í–‰",
    schedule_interval=None,
    start_date=days_ago(1),
    catchup=False,
    tags=["vectordb", "etl", "validate"],
) as validate_dag:
    
    validate_only_task = PythonOperator(
        task_id="validate_quality",
        python_callable=validate_quality,
        provide_context=True,
    )
