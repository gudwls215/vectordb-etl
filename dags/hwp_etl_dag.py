"""
VectorDB ETL Airflow DAG for HWP Documents
HWP ë¬¸ì„œë¥¼ Milvus ë²¡í„° DBì— ì €ì¥í•˜ëŠ” ETL íŒŒì´í”„ë¼ì¸
í´ë”ë³„ë¡œ ë³„ë„ì˜ ì»¬ë ‰ì…˜ì— ì €ì¥

DAG êµ¬ì¡°:
    extract_hwp_documents -> transform_to_chunks -> load_to_milvus_by_folder -> validate_quality

ì‚¬ìš©ë²•:
    1. ì´ íŒŒì¼ì„ Airflow dags ë””ë ‰í† ë¦¬ì— ë³µì‚¬
    2. VECTORDB_ETL_PATH í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë˜ëŠ” ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©)
    3. /hwp ë””ë ‰í† ë¦¬ì— í´ë”ë³„ë¡œ HWP íŒŒì¼ ë°°ì¹˜
    4. Airflow UIì—ì„œ DAG í™œì„±í™”
"""

from datetime import datetime, timedelta
from typing import Any, Dict, List
import os
import pickle
import json

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.utils.dates import days_ago

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
VECTORDB_ETL_PATH = os.environ.get(
    "VECTORDB_ETL_PATH", 
    os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
)

# ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
import sys
if VECTORDB_ETL_PATH not in sys.path:
    sys.path.insert(0, VECTORDB_ETL_PATH)


# DAG ê¸°ë³¸ ì¸ì
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    "execution_timeout": timedelta(hours=2),
}


def extract_hwp_documents(**context) -> str:
    """
    Extract ë‹¨ê³„: HWP íŒŒì¼ì—ì„œ ë¬¸ì„œ ì¶”ì¶œ
    
    Returns:
        ì €ì¥ëœ ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
    """
    from modules import load_hwp_documents, get_config, DATA_DIR
    
    config = get_config()
    
    print(f"HWP ë””ë ‰í† ë¦¬: {config.hwp_dir}")
    
    # HWP ë¬¸ì„œ ë¡œë“œ
    documents = load_hwp_documents(
        directory=config.hwp_dir,
        recursive=True,
        config=config
    )
    
    print(f"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
    
    # í´ë”ë³„ ë¬¸ì„œ ë¶„ë¥˜
    folder_documents = {}
    for doc in documents:
        folder_name = doc.metadata.get('folder_name', 'root')
        if folder_name not in folder_documents:
            folder_documents[folder_name] = []
        folder_documents[folder_name].append(doc)
    
    print("í´ë”ë³„ ë¬¸ì„œ ìˆ˜:")
    for folder, docs in folder_documents.items():
        print(f"  - {folder}: {len(docs)}ê°œ")
    
    # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
    documents_path = os.path.join(DATA_DIR, "hwp_documents.pkl")
    os.makedirs(DATA_DIR, exist_ok=True)
    
    with open(documents_path, 'wb') as f:
        pickle.dump(documents, f)
    
    # XComìœ¼ë¡œ ê²½ë¡œ ì „ë‹¬
    context['ti'].xcom_push(key='documents_path', value=documents_path)
    context['ti'].xcom_push(key='document_count', value=len(documents))
    context['ti'].xcom_push(key='folder_names', value=list(folder_documents.keys()))
    
    return documents_path


def transform_to_chunks(**context) -> str:
    """
    Transform ë‹¨ê³„: ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
    í´ë”ë³„ë¡œ ë¶„ë¥˜ ìœ ì§€
    
    Returns:
        ì €ì¥ëœ ì²­í¬ íŒŒì¼ ê²½ë¡œ
    """
    from modules import chunk_documents, get_config, DATA_DIR
    
    config = get_config()
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    documents_path = context['ti'].xcom_pull(
        key='documents_path', 
        task_ids='extract_hwp_documents'
    )
    
    with open(documents_path, 'rb') as f:
        documents = pickle.load(f)
    
    print(f"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
    
    # ì²­í‚¹
    chunks = chunk_documents(
        documents,
        config=config.chunker,
        remove_duplicates=True,
        similarity_threshold=config.duplicate_similarity_threshold
    )
    
    print(f"ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
    
    # í´ë”ë³„ ì²­í¬ ë¶„ë¥˜
    folder_chunks = {}
    for chunk in chunks:
        folder_name = chunk.metadata.get('folder_name', 'root')
        if folder_name not in folder_chunks:
            folder_chunks[folder_name] = []
        folder_chunks[folder_name].append(chunk)
    
    print("í´ë”ë³„ ì²­í¬ ìˆ˜:")
    for folder, ch in folder_chunks.items():
        print(f"  - {folder}: {len(ch)}ê°œ")
    
    # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
    chunks_path = os.path.join(DATA_DIR, "hwp_chunks.pkl")
    
    with open(chunks_path, 'wb') as f:
        pickle.dump(chunks, f)
    
    # XComìœ¼ë¡œ ê²½ë¡œ ì „ë‹¬
    context['ti'].xcom_push(key='chunks_path', value=chunks_path)
    context['ti'].xcom_push(key='chunk_count', value=len(chunks))
    context['ti'].xcom_push(key='folder_chunk_counts', value={k: len(v) for k, v in folder_chunks.items()})
    
    return chunks_path


def load_to_milvus_by_folder(**context) -> Dict[str, Any]:
    """
    Load ë‹¨ê³„: í´ë”ë³„ë¡œ ë³„ë„ì˜ Milvus ì»¬ë ‰ì…˜ì— ì €ì¥
    
    ì»¬ë ‰ì…˜ ì´ë¦„ ê·œì¹™: hwp_{í´ë”ëª…}
    ì˜ˆ: /hwp/contracts/ -> hwp_contracts
        /hwp/reports/   -> hwp_reports
    
    Returns:
        ì €ì¥ ê²°ê³¼ í†µê³„
    """
    from modules import get_config, DATA_DIR, MilvusVectorStore
    
    config = get_config()
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    chunks_path = context['ti'].xcom_pull(
        key='chunks_path', 
        task_ids='transform_to_chunks'
    )
    
    with open(chunks_path, 'rb') as f:
        chunks = pickle.load(f)
    
    print(f"ë¡œë“œëœ ì²­í¬ ìˆ˜: {len(chunks)}")
    
    # í´ë”ë³„ ì²­í¬ ë¶„ë¥˜
    folder_chunks = {}
    for chunk in chunks:
        folder_name = chunk.metadata.get('folder_name', 'root')
        if folder_name not in folder_chunks:
            folder_chunks[folder_name] = []
        folder_chunks[folder_name].append(chunk)
    
    # í´ë”ë³„ë¡œ ë³„ë„ ì»¬ë ‰ì…˜ì— ì €ì¥
    results = {}
    
    for folder_name, folder_chunk_list in folder_chunks.items():
        # ì»¬ë ‰ì…˜ ì´ë¦„ ìƒì„±
        collection_name = f"hwp_{folder_name.lower().replace('-', '_').replace(' ', '_')}"
        
        print(f"\ní´ë” '{folder_name}' -> ì»¬ë ‰ì…˜ '{collection_name}'")
        print(f"  ì²­í¬ ìˆ˜: {len(folder_chunk_list)}")
        
        # í•´ë‹¹ í´ë”ìš© ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        vectorstore = MilvusVectorStore(
            collection_name=collection_name,
            uri=config.milvus.uri,
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„± (ê¸°ì¡´ ì‚­ì œ)
        vectorstore.create_collection(drop_existing=True)
        
        # ë¬¸ì„œ ì‚½ì…
        inserted_count = vectorstore.insert_documents(folder_chunk_list)
        
        # í†µê³„ ì¡°íšŒ
        stats = vectorstore.get_collection_stats()
        
        results[folder_name] = {
            "collection_name": collection_name,
            "inserted_count": inserted_count,
            "total_vectors": stats.get("row_count"),
        }
        
        print(f"  ì €ì¥ ì™„ë£Œ: {inserted_count}ê°œ")
    
    print(f"\nì „ì²´ ì €ì¥ ì™„ë£Œ: {sum(r['inserted_count'] for r in results.values())}ê°œ")
    
    # XComìœ¼ë¡œ ê²°ê³¼ ì „ë‹¬
    context['ti'].xcom_push(key='load_stats', value=results)
    
    return results


def validate_quality(**context) -> Dict[str, Any]:
    """
    Validate ë‹¨ê³„: í´ë”ë³„ í’ˆì§ˆ ê²€ì¦
    
    Returns:
        ê²€ì¦ ë³´ê³ ì„œ
    """
    from modules import get_config, validate_pipeline, DATA_DIR, MilvusVectorStore
    
    config = get_config()
    
    # ì²­í¬ ë¡œë“œ
    chunks_path = context['ti'].xcom_pull(
        key='chunks_path', 
        task_ids='transform_to_chunks'
    )
    
    with open(chunks_path, 'rb') as f:
        chunks = pickle.load(f)
    
    # ì €ì¥ ê²°ê³¼ ë¡œë“œ
    load_stats = context['ti'].xcom_pull(
        key='load_stats', 
        task_ids='load_to_milvus_by_folder'
    )
    
    # í´ë”ë³„ ê²€ì¦
    reports = {}
    
    for folder_name, stats in load_stats.items():
        collection_name = stats['collection_name']
        
        # í•´ë‹¹ í´ë”ì˜ ì²­í¬ë§Œ í•„í„°ë§
        folder_chunks = [c for c in chunks if c.metadata.get('folder_name', 'root') == folder_name]
        
        # ë²¡í„° ì €ì¥ì†Œ ì—°ê²°
        vectorstore = MilvusVectorStore(
            collection_name=collection_name,
            uri=config.milvus.uri,
        )
        
        # ê°„ë‹¨í•œ ê²€ì¦: ìƒ˜í”Œ ì¿¼ë¦¬
        test_queries = ["ë‚´ìš© ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"]
        
        try:
            report = validate_pipeline(
                vectorstore=vectorstore,
                chunks=folder_chunks,
                test_queries=test_queries,
                sample_count=min(3, len(folder_chunks))
            )
            reports[folder_name] = report
        except Exception as e:
            print(f"í´ë” '{folder_name}' ê²€ì¦ ì‹¤íŒ¨: {e}")
            reports[folder_name] = {"error": str(e)}
    
    # ì „ì²´ ë³´ê³ ì„œ ì €ì¥
    report_path = os.path.join(
        DATA_DIR, 
        f"hwp_validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    )
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(reports, f, ensure_ascii=False, indent=2)
    
    print(f"ê²€ì¦ ë³´ê³ ì„œ ì €ì¥: {report_path}")
    
    # XComìœ¼ë¡œ ê²°ê³¼ ì „ë‹¬
    context['ti'].xcom_push(key='validation_reports', value=reports)
    context['ti'].xcom_push(key='report_path', value=report_path)
    
    return reports


def notify_completion(**context):
    """
    íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ì•Œë¦¼
    """
    # ëª¨ë“  ë‹¨ê³„ì˜ ê²°ê³¼ ìˆ˜ì§‘
    document_count = context['ti'].xcom_pull(
        key='document_count', 
        task_ids='extract_hwp_documents'
    )
    
    chunk_count = context['ti'].xcom_pull(
        key='chunk_count', 
        task_ids='transform_to_chunks'
    )
    
    folder_chunk_counts = context['ti'].xcom_pull(
        key='folder_chunk_counts', 
        task_ids='transform_to_chunks'
    )
    
    load_stats = context['ti'].xcom_pull(
        key='load_stats', 
        task_ids='load_to_milvus_by_folder'
    )
    
    # ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š HWP VectorDB ETL íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ìš”ì•½")
    print("=" * 60)
    print(f"ì´ ë¬¸ì„œ ìˆ˜: {document_count}")
    print(f"ì´ ì²­í¬ ìˆ˜: {chunk_count}")
    print("\ní´ë”ë³„ ì»¬ë ‰ì…˜:")
    
    for folder_name, stats in load_stats.items():
        print(f"  ğŸ“ {folder_name}")
        print(f"     ì»¬ë ‰ì…˜: {stats['collection_name']}")
        print(f"     ë²¡í„° ìˆ˜: {stats['total_vectors']}")
    
    print("=" * 60)


# DAG ì •ì˜ - HWP ì „ì²´ íŒŒì´í”„ë¼ì¸
with DAG(
    dag_id="vectordb_hwp_etl_pipeline",
    default_args=default_args,
    description="HWP ë¬¸ì„œë¥¼ í´ë”ë³„ë¡œ Milvus ë²¡í„° DBì— ì €ì¥í•˜ëŠ” ETL íŒŒì´í”„ë¼ì¸",
    schedule_interval="@daily",
    start_date=days_ago(1),
    catchup=False,
    tags=["vectordb", "etl", "milvus", "hwp", "embedding"],
    doc_md=__doc__,
) as dag:
    
    # ì‹œì‘ íƒœìŠ¤í¬
    start = EmptyOperator(task_id="start")
    
    # Extract íƒœìŠ¤í¬
    extract_task = PythonOperator(
        task_id="extract_hwp_documents",
        python_callable=extract_hwp_documents,
        provide_context=True,
    )
    
    # Transform íƒœìŠ¤í¬
    transform_task = PythonOperator(
        task_id="transform_to_chunks",
        python_callable=transform_to_chunks,
        provide_context=True,
    )
    
    # Load íƒœìŠ¤í¬ (í´ë”ë³„ ì»¬ë ‰ì…˜)
    load_task = PythonOperator(
        task_id="load_to_milvus_by_folder",
        python_callable=load_to_milvus_by_folder,
        provide_context=True,
    )
    
    # Validate íƒœìŠ¤í¬
    validate_task = PythonOperator(
        task_id="validate_quality",
        python_callable=validate_quality,
        provide_context=True,
    )
    
    # ì™„ë£Œ ì•Œë¦¼ íƒœìŠ¤í¬
    notify_task = PythonOperator(
        task_id="notify_completion",
        python_callable=notify_completion,
        provide_context=True,
    )
    
    # ì¢…ë£Œ íƒœìŠ¤í¬
    end = EmptyOperator(task_id="end")
    
    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì •ì˜
    start >> extract_task >> transform_task >> load_task >> validate_task >> notify_task >> end


# ê°œë³„ ë‹¨ê³„ ì‹¤í–‰ì„ ìœ„í•œ ì„œë¸Œ DAGë“¤
with DAG(
    dag_id="vectordb_hwp_extract_only",
    default_args=default_args,
    description="HWP ë¬¸ì„œ ì¶”ì¶œë§Œ ì‹¤í–‰",
    schedule_interval=None,
    start_date=days_ago(1),
    catchup=False,
    tags=["vectordb", "etl", "hwp", "extract"],
) as extract_dag:
    
    extract_only_task = PythonOperator(
        task_id="extract_hwp_documents",
        python_callable=extract_hwp_documents,
        provide_context=True,
    )


with DAG(
    dag_id="vectordb_hwp_load_only",
    default_args=default_args,
    description="HWP Milvus ì €ì¥ë§Œ ì‹¤í–‰ (transform ê²°ê³¼ í•„ìš”)",
    schedule_interval=None,
    start_date=days_ago(1),
    catchup=False,
    tags=["vectordb", "etl", "hwp", "load"],
) as load_dag:
    
    load_only_task = PythonOperator(
        task_id="load_to_milvus_by_folder",
        python_callable=load_to_milvus_by_folder,
        provide_context=True,
    )
