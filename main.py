#!/usr/bin/env python
"""
VectorDB ETL Pipeline CLI
ê° ë‹¨ê³„ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” CLI ìŠ¤í¬ë¦½íŠ¸

Usage:
    # ì „ì²´ íŒŒì´í”„ë¼ì¸
    python main.py --stage all
    
    # ë‹¨ê³„ë³„ ì‹¤í–‰
    python main.py --stage extract
    python main.py --stage transform
    python main.py --stage load
    python main.py --stage validate
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    python main.py --stage search --query "ì„œìš¸ ì‚¬ë¬´ì‹¤ ì£¼ì†Œ"
    
    # ë²¡í„° DB ì´ˆê¸°í™”
    python main.py --stage reset --confirm
"""

import argparse
import json
import os
import pickle
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, List

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from modules import (
    PipelineConfig,
    get_config,
    create_config,
    load_html_documents,
    chunk_documents,
    get_vector_store,
    reset_vector_store,
    get_embeddings,
    validate_pipeline,
    search_with_scores,
    print_search_results,
    create_rag_prompt,
    DATA_DIR,
)


class PipelineRunner:
    """ETL íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self, config: Optional[PipelineConfig] = None):
        self.config = config or get_config()
        self.data_dir = Path(self.config.data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.documents_path = self.data_dir / "documents.pkl"
        self.chunks_path = self.data_dir / "chunks.pkl"
        
    def extract(self) -> List:
        """
        Extract ë‹¨ê³„: HTML íŒŒì¼ì—ì„œ ë¬¸ì„œ ì¶”ì¶œ
        """
        print("\n" + "=" * 60)
        print("ğŸ“‚ EXTRACT: HTML íŒŒì¼ ë¡œë“œ")
        print("=" * 60)
        
        documents = load_html_documents(
            directory=self.config.html_dir,
            glob_pattern=self.config.html_glob_pattern,
            config=self.config
        )
        
        print(f"\në¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
        
        if documents:
            print(f"\nì²« ë²ˆì§¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:")
            for key, value in documents[0].metadata.items():
                print(f"  {key}: {value}")
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
        with open(self.documents_path, 'wb') as f:
            pickle.dump(documents, f)
        print(f"\në¬¸ì„œ ì €ì¥ ì™„ë£Œ: {self.documents_path}")
        
        return documents
    
    def transform(self, documents: Optional[List] = None) -> List:
        """
        Transform ë‹¨ê³„: ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
        """
        print("\n" + "=" * 60)
        print("ğŸ”„ TRANSFORM: ë¬¸ì„œ ì²­í‚¹")
        print("=" * 60)
        
        # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
        if documents is None:
            if self.documents_path.exists():
                with open(self.documents_path, 'rb') as f:
                    documents = pickle.load(f)
                print(f"ì €ì¥ëœ ë¬¸ì„œ ë¡œë“œ: {len(documents)}ê°œ")
            else:
                raise FileNotFoundError(
                    f"ë¬¸ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.documents_path}\n"
                    "ë¨¼ì € extract ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
                )
        
        # ì²­í‚¹
        chunks = chunk_documents(
            documents,
            config=self.config.chunker,
            remove_duplicates=True,
            similarity_threshold=self.config.duplicate_similarity_threshold
        )
        
        if chunks:
            print(f"\nìƒ˜í”Œ ì²­í¬ ë©”íƒ€ë°ì´í„°:")
            for key, value in chunks[0].metadata.items():
                print(f"  {key}: {value}")
        
        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
        with open(self.chunks_path, 'wb') as f:
            pickle.dump(chunks, f)
        print(f"\nì²­í¬ ì €ì¥ ì™„ë£Œ: {self.chunks_path}")
        
        return chunks
    
    def load(self, chunks: Optional[List] = None) -> None:
        """
        Load ë‹¨ê³„: Milvusì— ë²¡í„° ì €ì¥
        """
        print("\n" + "=" * 60)
        print("ğŸ’¾ LOAD: Milvus ë²¡í„° ì €ì¥")
        print("=" * 60)
        
        # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
        if chunks is None:
            if self.chunks_path.exists():
                with open(self.chunks_path, 'rb') as f:
                    chunks = pickle.load(f)
                print(f"ì €ì¥ëœ ì²­í¬ ë¡œë“œ: {len(chunks)}ê°œ")
            else:
                raise FileNotFoundError(
                    f"ì²­í¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.chunks_path}\n"
                    "ë¨¼ì € transform ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
                )
        
        # Milvusì— ì €ì¥
        vectorstore = get_vector_store()
        vectorstore.create_collection(drop_existing=True)
        vectorstore.insert_documents(chunks)
        
        # í†µê³„ ì¶œë ¥
        stats = vectorstore.get_collection_stats()
        print(f"\nì €ì¥ ì™„ë£Œ:")
        print(f"  ì»¬ë ‰ì…˜: {stats.get('collection_name', 'N/A')}")
        print(f"  ë²¡í„° ìˆ˜: {stats.get('row_count', 'N/A')}")
    
    def validate(self, chunks: Optional[List] = None) -> dict:
        """
        Validate ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦
        """
        print("\n" + "=" * 60)
        print("âœ… VALIDATE: í’ˆì§ˆ ê²€ì¦")
        print("=" * 60)
        
        # ì²­í¬ ë¡œë“œ
        if chunks is None:
            if self.chunks_path.exists():
                with open(self.chunks_path, 'rb') as f:
                    chunks = pickle.load(f)
                print(f"ì €ì¥ëœ ì²­í¬ ë¡œë“œ: {len(chunks)}ê°œ")
            else:
                raise FileNotFoundError(
                    f"ì²­í¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.chunks_path}\n"
                    "ë¨¼ì € transform ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
                )
        
        # ë²¡í„° ì €ì¥ì†Œ
        vectorstore = get_vector_store()
        
        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        test_queries = [
            "ì„œìš¸ ì‚¬ë¬´ì‹¤ ì£¼ì†Œ",
            "ìˆ˜ê°•ì‹ ì²­ë°©ë²•",
            "Seoul office address",
        ]
        
        # ê²€ì¦
        report = validate_pipeline(
            vectorstore=vectorstore,
            chunks=chunks,
            test_queries=test_queries,
            sample_count=3
        )
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = self.data_dir / f"validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"\nê²€ì¦ ë³´ê³ ì„œ ì €ì¥: {report_path}")
        
        return report
    
    def search(self, query: str, k: int = 3, language: Optional[str] = None) -> None:
        """
        ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        """
        print("\n" + "=" * 60)
        print("ğŸ” SEARCH: ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        print(f"\nì¿¼ë¦¬: '{query}'")
        if language:
            print(f"ì–¸ì–´ í•„í„°: {language}")
        
        results = search_with_scores(
            query=query,
            k=k,
            filter_language=language,
            auto_detect_language=(language is None)
        )
        
        print_search_results(results)
    
    def reset(self, confirm: bool = False) -> None:
        """
        ë²¡í„° DB ì´ˆê¸°í™”
        """
        print("\n" + "=" * 60)
        print("ğŸ—‘ï¸ RESET: ë²¡í„° DB ì´ˆê¸°í™”")
        print("=" * 60)
        
        vectorstore = get_vector_store()
        stats = vectorstore.get_collection_stats()
        
        if stats.get('exists'):
            print(f"\nì»¬ë ‰ì…˜: {stats.get('collection_name')}")
            print(f"ë²¡í„° ìˆ˜: {stats.get('row_count', 0)}")
            
            if confirm:
                vectorstore.drop_collection()
                print("\nâœ… ì»¬ë ‰ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print("\nâš ï¸ ì‚­ì œí•˜ë ¤ë©´ --confirm ì˜µì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”.")
        else:
            print("â„¹ï¸ ì‚­ì œí•  ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì¤‘ê°„ íŒŒì¼ ì‚­ì œ
        if confirm:
            for path in [self.documents_path, self.chunks_path]:
                if path.exists():
                    path.unlink()
                    print(f"ì‚­ì œë¨: {path}")
    
    def run_all(self) -> None:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        """
        print("\n" + "=" * 60)
        print("ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
        print("=" * 60)
        
        start_time = datetime.now()
        
        # Extract
        documents = self.extract()
        
        # Transform
        chunks = self.transform(documents)
        
        # Load
        self.load(chunks)
        
        # Validate
        self.validate(chunks)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        print("\n" + "=" * 60)
        print(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ)")
        print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="VectorDB ETL Pipeline CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # ì „ì²´ íŒŒì´í”„ë¼ì¸
    python main.py --stage all
    
    # ë‹¨ê³„ë³„ ì‹¤í–‰
    python main.py --stage extract
    python main.py --stage transform
    python main.py --stage load
    python main.py --stage validate
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    python main.py --stage search --query "ì„œìš¸ ì‚¬ë¬´ì‹¤ ì£¼ì†Œ"
    python main.py --stage search --query "address" --language english
    
    # ë²¡í„° DB ì´ˆê¸°í™”
    python main.py --stage reset --confirm
        """
    )
    
    parser.add_argument(
        "--stage",
        type=str,
        required=True,
        choices=["all", "extract", "transform", "load", "validate", "search", "reset"],
        help="ì‹¤í–‰í•  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„"
    )
    
    parser.add_argument(
        "--query",
        type=str,
        help="ê²€ìƒ‰ ì¿¼ë¦¬ (search ë‹¨ê³„ì—ì„œ ì‚¬ìš©)"
    )
    
    parser.add_argument(
        "--language",
        type=str,
        choices=["korean", "english", "vietnamese"],
        help="ê²€ìƒ‰ ì–¸ì–´ í•„í„° (search ë‹¨ê³„ì—ì„œ ì‚¬ìš©)"
    )
    
    parser.add_argument(
        "--k",
        type=int,
        default=3,
        help="ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 3)"
    )
    
    parser.add_argument(
        "--confirm",
        action="store_true",
        help="ì´ˆê¸°í™” í™•ì¸ (reset ë‹¨ê³„ì—ì„œ ì‚¬ìš©)"
    )
    
    parser.add_argument(
        "--html-dir",
        type=str,
        help="HTML íŒŒì¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--milvus-uri",
        type=str,
        help="Milvus URI (ê¸°ë³¸ê°’: ë¡œì»¬ íŒŒì¼)"
    )
    
    parser.add_argument(
        "--collection",
        type=str,
        default="html_documents",
        help="Milvus ì»¬ë ‰ì…˜ ì´ë¦„ (ê¸°ë³¸ê°’: html_documents)"
    )
    
    args = parser.parse_args()
    
    # ì„¤ì • ìƒì„±
    config_kwargs = {
        "milvus_uri": args.milvus_uri,
        "collection_name": args.collection,
    }
    if args.html_dir:
        config_kwargs["html_dir"] = args.html_dir
    
    config = create_config(**config_kwargs)
    
    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ê¸° ìƒì„±
    runner = PipelineRunner(config)
    
    # ë‹¨ê³„ ì‹¤í–‰
    if args.stage == "all":
        runner.run_all()
    elif args.stage == "extract":
        runner.extract()
    elif args.stage == "transform":
        runner.transform()
    elif args.stage == "load":
        runner.load()
    elif args.stage == "validate":
        runner.validate()
    elif args.stage == "search":
        if not args.query:
            parser.error("--query ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        runner.search(args.query, k=args.k, language=args.language)
    elif args.stage == "reset":
        runner.reset(confirm=args.confirm)


if __name__ == "__main__":
    main()
