"""
Quality Monitoring Module
"""
import random
from typing import List, Dict, Any, Tuple, Optional
from collections import Counter
import numpy as np

from langchain_core.documents import Document

from .milvus_store import MilvusVectorStore


class QualityMonitor:
    """ë²¡í„° DB í’ˆì§ˆ ê²€ì¦ ë° ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(
        self, 
        vectorstore: MilvusVectorStore, 
        chunks: List[Document]
    ):
        self.vectorstore = vectorstore
        self.chunks = chunks
        
    def analyze_chunk_distribution(self) -> Tuple[List[int], List[int]]:
        """ì²­í¬ í¬ê¸° ë¶„í¬ ë¶„ì„"""
        char_sizes = [len(c.page_content) for c in self.chunks]
        token_sizes = [c.metadata.get('chunk_size_tokens', 0) for c in self.chunks]
        
        print("=" * 50)
        print("ğŸ“Š ì²­í¬ í¬ê¸° ë¶„í¬ ë¶„ì„")
        print("=" * 50)
        print(f"\n[ë¬¸ì ìˆ˜ ê¸°ì¤€]")
        print(f"  ì´ ì²­í¬ ìˆ˜: {len(char_sizes)}")
        print(f"  ìµœì†Œ: {min(char_sizes)}")
        print(f"  ìµœëŒ€: {max(char_sizes)}")
        print(f"  í‰ê· : {np.mean(char_sizes):.1f}")
        print(f"  ì¤‘ì•™ê°’: {np.median(char_sizes):.1f}")
        print(f"  í‘œì¤€í¸ì°¨: {np.std(char_sizes):.1f}")
        
        print(f"\n[í† í° ìˆ˜ ê¸°ì¤€ (ì¶”ì •)]")
        print(f"  ìµœì†Œ: {min(token_sizes)}")
        print(f"  ìµœëŒ€: {max(token_sizes)}")
        print(f"  í‰ê· : {np.mean(token_sizes):.1f}")
        print(f"  ì¤‘ì•™ê°’: {np.median(token_sizes):.1f}")
        
        return char_sizes, token_sizes
    
    def plot_distribution(self, char_sizes: List[int], token_sizes: List[int]) -> None:
        """ì²­í¬ í¬ê¸° ë¶„í¬ ì‹œê°í™”"""
        try:
            import matplotlib.pyplot as plt
            
            fig, axes = plt.subplots(1, 2, figsize=(14, 5))
            
            axes[0].hist(char_sizes, bins=30, edgecolor='black', alpha=0.7)
            axes[0].set_xlabel('Characters')
            axes[0].set_ylabel('Frequency')
            axes[0].set_title('Chunk Size Distribution (Characters)')
            axes[0].axvline(np.mean(char_sizes), color='red', linestyle='--', 
                          label=f'Mean: {np.mean(char_sizes):.0f}')
            axes[0].legend()
            
            axes[1].hist(token_sizes, bins=30, edgecolor='black', alpha=0.7, color='green')
            axes[1].set_xlabel('Tokens (Estimated)')
            axes[1].set_ylabel('Frequency')
            axes[1].set_title('Chunk Size Distribution (Tokens)')
            axes[1].axvline(np.mean(token_sizes), color='red', linestyle='--', 
                          label=f'Mean: {np.mean(token_sizes):.0f}')
            axes[1].axvline(300, color='blue', linestyle=':', label='Target Range: 300-500')
            axes[1].axvline(500, color='blue', linestyle=':')
            axes[1].legend()
            
            plt.tight_layout()
            plt.show()
        except ImportError:
            print("matplotlibê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    def analyze_metadata(self) -> Tuple[Counter, Counter]:
        """ë©”íƒ€ë°ì´í„° ë¶„ì„"""
        print("\n" + "=" * 50)
        print("ğŸ“‹ ë©”íƒ€ë°ì´í„° ë¶„ì„")
        print("=" * 50)
        
        # ì–¸ì–´ ë¶„í¬
        languages = [c.metadata.get('language', 'unknown') for c in self.chunks]
        lang_counts = Counter(languages)
        print(f"\n[ì–¸ì–´ ë¶„í¬]")
        for lang, count in lang_counts.most_common():
            print(f"  {lang}: {count} ({count/len(self.chunks)*100:.1f}%)")
        
        # ì†ŒìŠ¤ íŒŒì¼ ë¶„í¬
        sources = [c.metadata.get('filename', 'unknown') for c in self.chunks]
        source_counts = Counter(sources)
        print(f"\n[ì†ŒìŠ¤ íŒŒì¼ë³„ ì²­í¬ ìˆ˜]")
        for source, count in source_counts.most_common(10):
            print(f"  {source}: {count}")
            
        return lang_counts, source_counts
    
    def test_search_quality(self, test_queries: List[str], k: int = 3) -> None:
        """ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""
        print("\n" + "=" * 50)
        print("ğŸ” ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸")
        print("=" * 50)
        
        for query in test_queries:
            print(f"\nì¿¼ë¦¬: '{query}'")
            print("-" * 40)
            
            results = self.vectorstore.search_with_scores(query, k=k)
            
            for i, (doc, score) in enumerate(results, 1):
                print(f"\n  [{i}] ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}")
                print(f"      ì†ŒìŠ¤: {doc.metadata.get('filename', 'N/A')}")
                print(f"      ì–¸ì–´: {doc.metadata.get('language', 'N/A')}")
                print(f"      ë‚´ìš©: {doc.page_content[:150]}...")
    
    def test_search_with_language_filter(
        self, 
        test_queries: List[Tuple[str, str]], 
        k: int = 3
    ) -> None:
        """ì–¸ì–´ë³„ í•„í„°ë§ì„ í¬í•¨í•œ ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""
        print("\n" + "=" * 50)
        print("ğŸ” ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ (ì–¸ì–´ í•„í„°ë§)")
        print("=" * 50)
        
        for query, lang in test_queries:
            print(f"\nì¿¼ë¦¬: '{query}' (ì–¸ì–´: {lang})")
            print("-" * 40)
            
            # ì–¸ì–´ í•„í„°ë§ ì ìš©
            filter_expr = f'language == "{lang}"' if lang else None
            results = self.vectorstore.search_with_scores(query, k=k, filter_expr=filter_expr)
            
            for i, (doc, score) in enumerate(results, 1):
                print(f"\n  [{i}] ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}")
                print(f"      ì†ŒìŠ¤: {doc.metadata.get('filename', 'N/A')}")
                print(f"      ì–¸ì–´: {doc.metadata.get('language', 'N/A')}")
                print(f"      ë‚´ìš©: {doc.page_content[:200]}...")
    
    def sample_chunks_review(self, n: int = 5) -> None:
        """ìƒ˜í”Œ ì²­í¬ ê²€í† """
        print("\n" + "=" * 50)
        print("ğŸ“ ìƒ˜í”Œ ì²­í¬ ê²€í† ")
        print("=" * 50)
        
        sample_indices = random.sample(range(len(self.chunks)), min(n, len(self.chunks)))
        
        for i, idx in enumerate(sample_indices, 1):
            chunk = self.chunks[idx]
            print(f"\n[ìƒ˜í”Œ {i}]")
            print(f"  ì†ŒìŠ¤: {chunk.metadata.get('filename', 'N/A')}")
            print(f"  ì²­í¬ ì¸ë±ìŠ¤: {chunk.metadata.get('chunk_index', 'N/A')}/{chunk.metadata.get('total_chunks', 'N/A')}")
            print(f"  í¬ê¸°: {len(chunk.page_content)} chars / {chunk.metadata.get('chunk_size_tokens', 'N/A')} tokens")
            print(f"  ë‚´ìš©:\n{chunk.page_content[:300]}...")
            print("-" * 40)
    
    def generate_report(self) -> Dict[str, Any]:
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "=" * 60)
        print("ğŸ“ˆ ë²¡í„° DB í’ˆì§ˆ ì¢…í•© ë³´ê³ ì„œ")
        print("=" * 60)
        
        stats = self.vectorstore.get_collection_stats()
        
        print(f"\n[ê¸°ë³¸ í†µê³„]")
        print(f"  ì´ ì²­í¬ ìˆ˜: {len(self.chunks)}")
        print(f"  ë²¡í„° DB ë¬¸ì„œ ìˆ˜: {stats.get('row_count', 'N/A')}")
        
        char_sizes = [len(c.page_content) for c in self.chunks]
        token_sizes = [c.metadata.get('chunk_size_tokens', 0) for c in self.chunks]
        
        # ëª©í‘œ ë²”ìœ„ (300~500 í† í°) ë‚´ ì²­í¬ ë¹„ìœ¨
        in_range = sum(1 for t in token_sizes if 300 <= t <= 500)
        in_range_ratio = in_range / len(token_sizes) * 100 if token_sizes else 0
        
        print(f"\n[í’ˆì§ˆ ì§€í‘œ]")
        print(f"  ëª©í‘œ í† í° ë²”ìœ„ (300-500) ë‚´ ì²­í¬ ë¹„ìœ¨: {in_range_ratio:.1f}%")
        print(f"  í‰ê·  ì²­í¬ í¬ê¸°: {np.mean(char_sizes):.0f} chars / {np.mean(token_sizes):.0f} tokens")
        
        cv = np.std(token_sizes)/np.mean(token_sizes)*100 if np.mean(token_sizes) > 0 else 0
        print(f"  ì²­í¬ í¬ê¸° ì¼ê´€ì„± (CV): {cv:.1f}%")
        
        # ê¶Œì¥ ì‚¬í•­
        print(f"\n[ê¶Œì¥ ì‚¬í•­]")
        recommendations = []
        if in_range_ratio < 70:
            msg = "âš ï¸ ëª©í‘œ ë²”ìœ„ ë‚´ ì²­í¬ ë¹„ìœ¨ì´ ë‚®ìŠµë‹ˆë‹¤. chunk_size íŒŒë¼ë¯¸í„° ì¡°ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
            print(f"  {msg}")
            recommendations.append(msg)
        else:
            msg = "âœ… ì²­í¬ í¬ê¸° ë¶„í¬ê°€ ì–‘í˜¸í•©ë‹ˆë‹¤."
            print(f"  {msg}")
            recommendations.append(msg)
            
        if cv > 50:
            msg = "âš ï¸ ì²­í¬ í¬ê¸° ë³€ë™ì´ í½ë‹ˆë‹¤. ë¶„í•  ì „ëµ ê²€í† ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
            print(f"  {msg}")
            recommendations.append(msg)
        else:
            msg = "âœ… ì²­í¬ í¬ê¸°ê°€ ì¼ê´€ì ì…ë‹ˆë‹¤."
            print(f"  {msg}")
            recommendations.append(msg)
        
        return {
            "total_chunks": len(self.chunks),
            "vector_count": stats.get('row_count', 0),
            "in_range_ratio": in_range_ratio,
            "avg_char_size": np.mean(char_sizes),
            "avg_token_size": np.mean(token_sizes),
            "cv": cv,
            "recommendations": recommendations,
        }


def validate_pipeline(
    vectorstore: MilvusVectorStore,
    chunks: List[Document],
    test_queries: Optional[List[str]] = None,
    sample_count: int = 3
) -> Dict[str, Any]:
    """íŒŒì´í”„ë¼ì¸ ê²€ì¦ í¸ì˜ í•¨ìˆ˜"""
    monitor = QualityMonitor(vectorstore, chunks)
    
    # ë¶„í¬ ë¶„ì„
    char_sizes, token_sizes = monitor.analyze_chunk_distribution()
    
    # ë©”íƒ€ë°ì´í„° ë¶„ì„
    monitor.analyze_metadata()
    
    # ìƒ˜í”Œ ê²€í† 
    monitor.sample_chunks_review(n=sample_count)
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    if test_queries:
        monitor.test_search_quality(test_queries)
    
    # ë³´ê³ ì„œ ìƒì„±
    report = monitor.generate_report()
    
    return report
