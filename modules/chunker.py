"""
Hybrid Document Chunker Module
1ë‹¨ê³„: ì‹œë§¨í‹± ë¶„í•  (ì˜ë¯¸ ë‹¨ìœ„)
2ë‹¨ê³„: ë¬¸ì ê¸°ì¤€ ë¶„í• /ë³‘í•© (í¬ê¸° ìµœì í™”)
"""
import re
import hashlib
from typing import List, Optional

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

try:
    from langchain_experimental.text_splitter import SemanticChunker as LangChainSemanticChunker
    SEMANTIC_AVAILABLE = True
except ImportError:
    SEMANTIC_AVAILABLE = False
    print("âš ï¸ langchain-experimental ë¯¸ì„¤ì¹˜. ë¬¸ì ê¸°ì¤€ ì²­í‚¹ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    print("   ì„¤ì¹˜: pip install langchain-experimental")

from .config import ChunkerConfig, get_config, EmbeddingConfig


class HybridChunker:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë¬¸ì„œ ë¶„í•  í´ë˜ìŠ¤
    
    1ë‹¨ê³„: ì‹œë§¨í‹± ì²­í‚¹ - ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì¥ë“¤ì„ ê·¸ë£¹í™”
    2ë‹¨ê³„: í¬ê¸° ìµœì í™” - max_chunk_size ì´ˆê³¼ì‹œ ë¶„í• , min_chunk_size ë¯¸ë§Œì‹œ ë³‘í•©
    """
    
    def __init__(self, config: Optional[ChunkerConfig] = None, embedding_config: Optional[EmbeddingConfig] = None):
        self.config = config or get_config().chunker
        self.embedding_config = embedding_config or get_config().embedding
        
        # ë¬¸ì ê¸°ì¤€ ë¶„í• ê¸° (2ë‹¨ê³„ìš©)
        self.char_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.target_chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=self.config.separators,
            length_function=len,
            is_separator_regex=False
        )
        
        # ì‹œë§¨í‹± ë¶„í• ê¸° (1ë‹¨ê³„ìš©)
        self.semantic_splitter = None
        if self.config.chunking_mode == "semantic_first" and SEMANTIC_AVAILABLE:
            try:
                from .embeddings import get_embeddings
                embeddings = get_embeddings(self.embedding_config)
                self.semantic_splitter = LangChainSemanticChunker(
                    embeddings=embeddings,
                    breakpoint_threshold_type=self.config.breakpoint_threshold_type,
                    breakpoint_threshold_amount=self.config.breakpoint_threshold_amount,
                )
                print(f"ğŸ§  í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ í™œì„±í™” (1ë‹¨ê³„: ì‹œë§¨í‹± â†’ 2ë‹¨ê³„: ë¬¸ì ê¸°ì¤€)")
                print(f"   ì‹œë§¨í‹±: {self.config.breakpoint_threshold_type}={self.config.breakpoint_threshold_amount}")
                print(f"   ë¬¸ì: target={self.config.target_chunk_size}, min={self.config.min_chunk_size}, max={self.config.max_chunk_size}")
            except Exception as e:
                print(f"âš ï¸ ì‹œë§¨í‹± ì²­í‚¹ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                print("   ë¬¸ì ê¸°ì¤€ ì²­í‚¹ìœ¼ë¡œ fallback")
        else:
            print(f"ğŸ“ ë¬¸ì ê¸°ì¤€ ì²­í‚¹ ì‚¬ìš© (size={self.config.target_chunk_size}, overlap={self.config.chunk_overlap})")
    
    def _estimate_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì • (í•œê¸€ ê³ ë ¤)"""
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        other_chars = len(text) - korean_chars
        return int(korean_chars / 1.5 + other_chars / 4)
    
    def _split_large_chunk(self, chunk: Document) -> List[Document]:
        """max_chunk_size ì´ˆê³¼ ì²­í¬ë¥¼ ë¬¸ì ê¸°ì¤€ìœ¼ë¡œ ë¶„í• """
        return self.char_splitter.split_documents([chunk])
    
    def _merge_small_chunks(self, chunks: List[Document]) -> List[Document]:
        """
        min_chunk_size ë¯¸ë§Œ ì²­í¬ë¥¼ ì¸ì ‘ ì²­í¬ì™€ ë³‘í•©
        ì˜ë¯¸ì  ì—°ì†ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ë³‘í•©
        """
        if not chunks:
            return chunks
        
        merged = []
        buffer = None
        
        for chunk in chunks:
            chunk_len = len(chunk.page_content)
            
            if buffer is None:
                buffer = chunk
            elif len(buffer.page_content) < self.config.min_chunk_size:
                # ë²„í¼ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ í˜„ì¬ ì²­í¬ì™€ ë³‘í•©
                merged_content = buffer.page_content + "\n" + chunk.page_content
                
                # ë³‘í•©í•´ë„ maxë¥¼ ë„˜ì§€ ì•Šìœ¼ë©´ ë³‘í•©
                if len(merged_content) <= self.config.max_chunk_size:
                    buffer = Document(
                        page_content=merged_content,
                        metadata=buffer.metadata.copy()
                    )
                else:
                    # ë³‘í•©í•˜ë©´ ë„ˆë¬´ ì»¤ì§€ë¯€ë¡œ ë²„í¼ ì €ì¥ í›„ ìƒˆ ë²„í¼ ì‹œì‘
                    merged.append(buffer)
                    buffer = chunk
            elif chunk_len < self.config.min_chunk_size:
                # í˜„ì¬ ì²­í¬ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ë²„í¼ì™€ ë³‘í•© ì‹œë„
                merged_content = buffer.page_content + "\n" + chunk.page_content
                
                if len(merged_content) <= self.config.max_chunk_size:
                    buffer = Document(
                        page_content=merged_content,
                        metadata=buffer.metadata.copy()
                    )
                else:
                    merged.append(buffer)
                    buffer = chunk
            else:
                # ë‘˜ ë‹¤ ì ì • í¬ê¸°
                merged.append(buffer)
                buffer = chunk
        
        if buffer is not None:
            merged.append(buffer)
        
        return merged
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì²­í¬ ë¶„í• """
        all_chunks = []
        
        for doc in documents:
            # 1ë‹¨ê³„: ì‹œë§¨í‹± ë¶„í•  (ê°€ëŠ¥í•œ ê²½ìš°)
            if self.semantic_splitter:
                try:
                    semantic_chunks = self.semantic_splitter.split_documents([doc])
                    print(f"   ğŸ“Š 1ë‹¨ê³„ ì‹œë§¨í‹± ë¶„í• : {len(semantic_chunks)}ê°œ ì²­í¬")
                except Exception as e:
                    print(f"   âš ï¸ ì‹œë§¨í‹± ë¶„í•  ì‹¤íŒ¨, ë¬¸ì ê¸°ì¤€ìœ¼ë¡œ fallback: {e}")
                    semantic_chunks = [doc]
            else:
                semantic_chunks = [doc]
            
            # 2ë‹¨ê³„: í¬ê¸° ìµœì í™”
            optimized_chunks = []
            for chunk in semantic_chunks:
                chunk_len = len(chunk.page_content)
                
                if chunk_len > self.config.max_chunk_size:
                    # ë„ˆë¬´ í¬ë©´ ë¬¸ì ê¸°ì¤€ìœ¼ë¡œ ì¶”ê°€ ë¶„í• 
                    sub_chunks = self._split_large_chunk(chunk)
                    optimized_chunks.extend(sub_chunks)
                else:
                    optimized_chunks.append(chunk)
            
            # 3ë‹¨ê³„: ì‘ì€ ì²­í¬ ë³‘í•©
            final_chunks = self._merge_small_chunks(optimized_chunks)
            
            if self.semantic_splitter:
                print(f"   ğŸ“Š 2ë‹¨ê³„ í¬ê¸° ìµœì í™” í›„: {len(final_chunks)}ê°œ ì²­í¬")
            
            # ë©”íƒ€ë°ì´í„° ë¶€ì—¬
            for i, chunk in enumerate(final_chunks):
                chunk.metadata['chunk_index'] = i
                chunk.metadata['total_chunks'] = len(final_chunks)
                chunk.metadata['chunk_size_chars'] = len(chunk.page_content)
                chunk.metadata['chunk_size_tokens'] = self._estimate_tokens(chunk.page_content)
                
                chunk_id = hashlib.md5(
                    f"{chunk.metadata.get('source', 'unknown')}_{i}_{chunk.page_content[:50]}".encode()
                ).hexdigest()[:12]
                chunk.metadata['chunk_id'] = chunk_id
            
            all_chunks.extend(final_chunks)
        
        return all_chunks


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
SemanticChunker = HybridChunker


def remove_duplicate_chunks(
    chunks: List[Document], 
    similarity_threshold: float = 0.95
) -> List[Document]:
    """ì¤‘ë³µ ì²­í¬ ì œê±° (í•´ì‹œ ê¸°ë°˜)"""
    
    seen_hashes = set()
    unique_chunks = []
    duplicate_count = 0
    
    for chunk in chunks:
        # ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ì˜ í•´ì‹œ ìƒì„±
        normalized_text = ' '.join(chunk.page_content.lower().split())
        text_hash = hashlib.md5(normalized_text.encode()).hexdigest()
        
        if text_hash not in seen_hashes:
            seen_hashes.add(text_hash)
            unique_chunks.append(chunk)
        else:
            duplicate_count += 1
    
    print(f"ì›ë³¸ ì²­í¬ ìˆ˜: {len(chunks)}")
    print(f"ì¤‘ë³µ ì œê±°ëœ ì²­í¬ ìˆ˜: {duplicate_count}")
    print(f"ìµœì¢… ì²­í¬ ìˆ˜: {len(unique_chunks)}")
    
    return unique_chunks


def chunk_documents(
    documents: List[Document],
    config: Optional[ChunkerConfig] = None,
    remove_duplicates: bool = True,
    similarity_threshold: float = 0.95
) -> List[Document]:
    """ë¬¸ì„œ ì²­í‚¹ í¸ì˜ í•¨ìˆ˜"""
    chunker = SemanticChunker(config)
    chunks = chunker.split_documents(documents)
    
    print(f"ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(chunks)}")
    if len(documents) > 0:
        print(f"ë¬¸ì„œë‹¹ í‰ê·  ì²­í¬ ìˆ˜: {len(chunks) / len(documents):.1f}")
    
    if remove_duplicates:
        chunks = remove_duplicate_chunks(chunks, similarity_threshold)
    
    return chunks
